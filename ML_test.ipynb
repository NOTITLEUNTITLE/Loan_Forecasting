{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPnG1X0tZMtk","executionInfo":{"status":"ok","timestamp":1645364979686,"user_tz":-540,"elapsed":20368,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}},"outputId":"9bdc5acd-70f4-4183-e3fd-31d38073554e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# library import"],"metadata":{"id":"F5kNI3X_a43a"}},{"cell_type":"code","execution_count":21,"metadata":{"id":"f4c_lLv9zNPE","executionInfo":{"status":"ok","timestamp":1645367262392,"user_tz":-540,"elapsed":350,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.experimental import enable_hist_gradient_boosting \n","from sklearn.ensemble import HistGradientBoostingClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","\n","\n","from xgboost import XGBClassifier\n","from catboost import CatBoostClassifier\n","from lightgbm import LGBMClassifier\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/PyTorch_YearDream/2022-01-26/data/train.csv')"],"metadata":{"id":"7uBEzGqzZe8r","executionInfo":{"status":"ok","timestamp":1645365079166,"user_tz":-540,"elapsed":2558,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# EDA\n","- EDA는 pr_report.html 을 참고해주세요!!\n"],"metadata":{"id":"XL4L4MO3dIm9"}},{"cell_type":"markdown","source":["# data Split"],"metadata":{"id":"D34vIlEudXlL"}},{"cell_type":"code","source":["X = df.drop('depvar', axis=1)\n","y = df['depvar']\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"],"metadata":{"id":"gXLkrgP_ZkGy","executionInfo":{"status":"ok","timestamp":1645365089306,"user_tz":-540,"elapsed":339,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"g40Hh_sjg4_f","executionInfo":{"status":"ok","timestamp":1645367008472,"user_tz":-540,"elapsed":347,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"EYhj9S8L1xFE","executionInfo":{"status":"ok","timestamp":1645365090014,"user_tz":-540,"elapsed":368,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}}},"outputs":[],"source":["length = int(len(X)/5)\n","\n","X_train1 = X[ : length]\n","X_train2 = X[length : length * 2]\n","X_train3 = X[length * 2 : length * 3]\n","X_train4 = X[length * 3 : length * 4]\n","X_train5 = X[length * 4 :]\n","\n","y_train1 = y[ : length]\n","y_train2 = y[length : length * 2]\n","y_train3 = y[length * 2 : length * 3]\n","y_train4 = y[length * 3 : length * 4]\n","y_train5 = y[length * 4 :]\n","\n","\n","frames = [X_train1, X_train2, X_train3, X_train4] # 5\n","X_train_dataset1 = pd.concat(frames)\n","\n","frames = [X_train1, X_train2, X_train3, X_train5] # 4\n","X_train_dataset2 = pd.concat(frames)\n","\n","frames = [X_train1, X_train2, X_train4, X_train5] # 3\n","X_train_dataset3 = pd.concat(frames)\n","\n","frames = [X_train1, X_train3, X_train4, X_train5] # 2\n","X_train_dataset4 = pd.concat(frames)\n","\n","frames = [X_train2, X_train3, X_train4, X_train5] # 1\n","X_train_dataset5 = pd.concat(frames)\n","\n","\n","frames = [y_train1, y_train2, y_train3, y_train4] # 5\n","y_train_dataset1 = pd.concat(frames)\n","\n","frames = [y_train1, y_train2, y_train3, y_train5] # 4\n","y_train_dataset2 = pd.concat(frames)\n","\n","frames = [y_train1, y_train2, y_train4, y_train5] # 3\n","y_train_dataset3 = pd.concat(frames)\n","\n","frames = [y_train1, y_train3, y_train4, y_train5] # 2\n","y_train_dataset4 = pd.concat(frames)\n","\n","frames = [y_train2, y_train3, y_train4, y_train5] # 1\n","y_train_dataset5 = pd.concat(frames)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ogNvS2GM10Gh","executionInfo":{"status":"ok","timestamp":1645365106062,"user_tz":-540,"elapsed":367,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}}},"outputs":[],"source":["# f1 score와 accuracy score 두개를 고려하였습니다.\n","# 두 score의 값을 더해서 평균을 낸 수치가 높은값을 찾기위한 함수 정의입니다.\n","\n","def calc_sum_f1_and_accuracy(y_actual, y_pred):\n","    return (accuracy_score(y_actual, y_pred) + f1_score(y_actual, y_pred)) / 2"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"letrXfIQY9vJ","executionInfo":{"status":"ok","timestamp":1645365124828,"user_tz":-540,"elapsed":334,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}}},"outputs":[],"source":["# threshold값을 최적화 시키는 함수.\n","# 어떤것을 기준으로 최적화 시키는 것이냐면, 위에서 정의된 calc_sum_f1_and_accuracy()의 값이 제일 높은 경우를 찾으려고 합니다.\n","\n","def calc_score_model(model, name, X_train, y_train, X_val, y_val):\n","    model1 = model\n","    model1.fit(X_train, y_train)\n","\n","    y_pred1 = model1.predict(X_val)\n","    y_prob1 = model1.predict_proba(X_val)\n","\n","    thr_result = 0.5\n","    max_val = 0.0\n","\n","    scale = 1000\n","    # 반목문을 통하여 threshold값을 0.5 ~ 0.0005까지 조정하면서 calc_sum_f1_and_accuracy()값이 제일 높은 threshold값을 찾습니다.\n","    for thr in range(1, scale):\n","        val = calc_sum_f1_and_accuracy(y_val, (y_prob1[:,1] >= thr / scale))\n","        if val > max_val:\n","            thr_result = thr / scale\n","            max_val = val\n","    \n","    return [name, thr_result, max_val, model]\n","\n"]},{"cell_type":"code","source":["# method test\n","print(calc_score_model(XGBClassifier(), 'XGB', X_train_dataset1, y_train_dataset1, X_train5, y_train5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZB6FFn7Zz2p","executionInfo":{"status":"ok","timestamp":1645365164446,"user_tz":-540,"elapsed":27769,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}},"outputId":"6890344d-658c-4d81-a306-906ef0fa762d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['XGB', 0.362, 0.6671189032723408, XGBClassifier()]\n"]}]},{"cell_type":"markdown","source":["# 각 모델별 최적화"],"metadata":{"id":"5KfIQv3DhFWH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjS4B2RQY9vL"},"outputs":[],"source":["# 여기에서 사용되는 set_dict는 위에서 정의한 train data의 개수와 일치하게 만듭니다.\n","# 단, model의 갯수는 5개를 넘어도 상관없습니다.\n","\n","\n","set1_dict = {}\n","        \n","# name, thr, val, model = calc_score_model(XGBClassifier(), 'XGB', X_train_dataset1, y_train_dataset1, X_train5, y_train5)\n","# set1_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(CatBoostClassifier(), 'Cat', X_train_dataset1, y_train_dataset1, X_train5, y_train5)\n","set1_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(LGBMClassifier(), 'LGBM', X_train_dataset1, y_train_dataset1, X_train5, y_train5)\n","set1_dict[name] = [val, thr, model]\n","\n","\n","name, thr, val, model = calc_score_model(RandomForestClassifier(), 'rnd', X_train_dataset1, y_train_dataset1, X_train5, y_train5)\n","set1_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(DecisionTreeClassifier(), 'Decision', X_train_dataset1, y_train_dataset1, X_train5, y_train5)\n","set1_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(GradientBoostingClassifier(), 'Gradient', X_train_dataset1, y_train_dataset1, X_train5, y_train5)\n","set1_dict[name] = [val, thr, model]\n","\n","\n","# name, thr, val, model = calc_score_model(HistGradientBoostingClassifier(), 'Hist', X_train_dataset1, y_train_dataset1, X_train5, y_train5)\n","# set1_dict[name] = [val, thr, model]\n","\n","# name, thr, val, model = calc_score_model(BaggingClassifier(), 'Bag', X_train_dataset1, y_train_dataset1, X_train5, y_train5)\n","# set1_dict[name] = [val, thr, model]\n","\n","\n","\n","\n","\n","set2_dict = {}\n","        \n","# name, thr, val, model = calc_score_model(XGBClassifier(), 'XGB', X_train_dataset2, y_train_dataset2, X_train4, y_train4)\n","# set2_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(CatBoostClassifier(), 'Cat', X_train_dataset2, y_train_dataset2, X_train4, y_train4)\n","set2_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(LGBMClassifier(), 'LGBM', X_train_dataset2, y_train_dataset2, X_train4, y_train4)\n","set2_dict[name] = [val, thr, model]\n","\n","\n","name, thr, val, model = calc_score_model(RandomForestClassifier(), 'rnd', X_train_dataset2, y_train_dataset2, X_train4, y_train4)\n","set2_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(DecisionTreeClassifier(), 'Decision', X_train_dataset2, y_train_dataset2, X_train4, y_train4)\n","set2_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(GradientBoostingClassifier(), 'Gradient', X_train_dataset2, y_train_dataset2, X_train4, y_train4)\n","set2_dict[name] = [val, thr, model]\n","\n","\n","# name, thr, val, model = calc_score_model(HistGradientBoostingClassifier(), 'Hist', X_train_dataset2, y_train_dataset2, X_train4, y_train4)\n","# set2_dict[name] = [val, thr, model]\n","\n","# name, thr, val, model = calc_score_model(BaggingClassifier(), 'Bag', X_train_dataset2, y_train_dataset2, X_train4, y_train4)\n","# set2_dict[name] = [val, thr, model]\n","\n","\n","\n","set3_dict = {}\n","        \n","# name, thr, val, model = calc_score_model(XGBClassifier(), 'XGB', X_train_dataset3, y_train_dataset3, X_train3, y_train3)\n","# set3_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(CatBoostClassifier(), 'Cat', X_train_dataset3, y_train_dataset3, X_train3, y_train3)\n","set3_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(LGBMClassifier(), 'LGBM', X_train_dataset3, y_train_dataset3, X_train3, y_train3)\n","set3_dict[name] = [val, thr, model]\n","\n","\n","name, thr, val, model = calc_score_model(RandomForestClassifier(), 'rnd', X_train_dataset3, y_train_dataset3, X_train3, y_train3)\n","set3_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(DecisionTreeClassifier(), 'Decision', X_train_dataset3, y_train_dataset3, X_train3, y_train3)\n","set3_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(GradientBoostingClassifier(), 'Gradient', X_train_dataset3, y_train_dataset3, X_train3, y_train3)\n","set3_dict[name] = [val, thr, model]\n","\n","\n","# name, thr, val, model = calc_score_model(HistGradientBoostingClassifier(), 'Hist',  X_train_dataset3, y_train_dataset3, X_train3, y_train3)\n","# set3_dict[name] = [val, thr, model]\n","\n","# name, thr, val, model = calc_score_model(BaggingClassifier(), 'Bag',  X_train_dataset3, y_train_dataset3, X_train3, y_train3)\n","# set3_dict[name] = [val, thr, model]\n","\n","\n","\n","\n","set4_dict = {}\n","        \n","# name, thr, val, model = calc_score_model(XGBClassifier(), 'XGB', X_train_dataset4, y_train_dataset4, X_train2, y_train2)\n","# set4_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(CatBoostClassifier(), 'Cat', X_train_dataset4, y_train_dataset4, X_train2, y_train2)\n","set4_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(LGBMClassifier(), 'LGBM', X_train_dataset4, y_train_dataset4, X_train2, y_train2)\n","set4_dict[name] = [val, thr, model]\n","\n","\n","name, thr, val, model = calc_score_model(RandomForestClassifier(), 'rnd', X_train_dataset4, y_train_dataset4, X_train2, y_train2)\n","set4_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(DecisionTreeClassifier(), 'Decision', X_train_dataset4, y_train_dataset4, X_train2, y_train2)\n","set4_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(GradientBoostingClassifier(), 'Gradient', X_train_dataset4, y_train_dataset4, X_train2, y_train2)\n","set4_dict[name] = [val, thr, model]\n","\n","\n","# name, thr, val, model = calc_score_model(HistGradientBoostingClassifier(), 'Hist',  X_train_dataset4, y_train_dataset4, X_train2, y_train2)\n","# set4_dict[name] = [val, thr, model]\n","\n","# name, thr, val, model = calc_score_model(BaggingClassifier(), 'Bag',  X_train_dataset4, y_train_dataset4, X_train2, y_train2)\n","# set4_dict[name] = [val, thr, model]\n","\n","\n","\n","set5_dict = {}\n","        \n","# name, thr, val, model = calc_score_model(XGBClassifier(), 'XGB', X_train_dataset5, y_train_dataset5, X_train1, y_train1)\n","# set5_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(CatBoostClassifier(), 'Cat', X_train_dataset5, y_train_dataset5, X_train1, y_train1)\n","set5_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(LGBMClassifier(), 'LGBM', X_train_dataset5, y_train_dataset5, X_train1, y_train1)\n","set5_dict[name] = [val, thr, model]\n","\n","\n","name, thr, val, model = calc_score_model(RandomForestClassifier(), 'rnd', X_train_dataset5, y_train_dataset5, X_train1, y_train1)\n","set5_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(DecisionTreeClassifier(), 'Decision', X_train_dataset5, y_train_dataset5, X_train1, y_train1)\n","set5_dict[name] = [val, thr, model]\n","\n","name, thr, val, model = calc_score_model(GradientBoostingClassifier(), 'Gradient', X_train_dataset5, y_train_dataset5, X_train1, y_train1)\n","set5_dict[name] = [val, thr, model]\n","\n","\n","# name, thr, val, model = calc_score_model(HistGradientBoostingClassifier(), 'Hist',  X_train_dataset5, y_train_dataset5, X_train1, y_train1)\n","# set5_dict[name] = [val, thr, model]\n","\n","# name, thr, val, model = calc_score_model(BaggingClassifier(), 'Bag',  X_train_dataset5, y_train_dataset5, X_train1, y_train1)\n","# set5_dict[name] = [val, thr, model]"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pwoGDKU9Y9vM","executionInfo":{"status":"ok","timestamp":1645366410787,"user_tz":-540,"elapsed":15,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}},"outputId":"b1d3362b-624e-4113-a88e-d516fbd5e79f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cat 0.687737265883217 0.363\n","LGBM 0.68583139448173 0.368\n","Gradient 0.6675123797244247 0.366\n","rnd 0.6484557297215036 0.381\n","Decision 0.5765703389830508 0.001\n","-------------\n","Cat 0.6897985839740425 0.36\n","LGBM 0.6868351554439782 0.342\n","Gradient 0.6698568019093079 0.366\n","rnd 0.6574241057158079 0.391\n","Decision 0.578490976771888 0.001\n","-------------\n","LGBM 0.6903328657056704 0.372\n","Cat 0.6884725601004664 0.357\n","Gradient 0.6671463209530484 0.335\n","rnd 0.6550289702233251 0.381\n","Decision 0.579489700770742 0.001\n","-------------\n","Cat 0.6875170129689026 0.333\n","LGBM 0.6852640684410647 0.391\n","Gradient 0.6638816281441196 0.367\n","rnd 0.6566285560913132 0.381\n","Decision 0.5780770874675623 0.001\n","-------------\n","Cat 0.6864923592493297 0.31\n","LGBM 0.6862856898517673 0.345\n","Gradient 0.668553090092721 0.349\n","rnd 0.6535108073505047 0.381\n","Decision 0.5826743902903742 0.001\n","-------------\n"]}],"source":["result1 = sorted(set1_dict.items(), key=lambda x: x[1][0], reverse=True)\n","for k, v in result1:\n","    print(k, v[0], v[1])    \n","print(\"-------------\")\n","\n","result2 = sorted(set2_dict.items(), key=lambda x: x[1][0], reverse=True)\n","for k, v in result2:\n","    print(k, v[0], v[1])     \n","print(\"-------------\")\n","\n","result3 = sorted(set3_dict.items(), key=lambda x: x[1][0], reverse=True)\n","for k, v in result3:\n","    print(k, v[0], v[1])       \n","print(\"-------------\")\n","\n","result4 = sorted(set4_dict.items(), key=lambda x: x[1][0], reverse=True)\n","for k, v in result4:\n","    print(k, v[0], v[1])    \n","print(\"-------------\")\n","\n","result5 = sorted(set5_dict.items(), key=lambda x: x[1][0], reverse=True)\n","for k, v in result5:\n","    print(k, v[0], v[1])    \n","print(\"-------------\")"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uDeMLGbbY9vP","executionInfo":{"status":"ok","timestamp":1645367211709,"user_tz":-540,"elapsed":9126,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}},"outputId":"9c4e89ec-ee9e-4bbd-ff56-0808629a78e9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":18}],"source":["submit = pd.read_csv('/content/drive/MyDrive/PyTorch_YearDream/2022-01-26/data/sample_submission.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/PyTorch_YearDream/2022-01-26/data/test.csv')\n","test = df_test.drop('ID', axis=1)\n","\n","\n","li = []\n","\n","for i in range(len(result1)):\n","    prob = result1[i][1][2].predict_proba(test)\n","    li.append((prob[:,1] >= result1[i][1][1]).astype(np.int64))\n","    \n","    prob = result2[i][1][2].predict_proba(test)\n","    li.append((prob[:,1] >= result2[i][1][1]).astype(np.int64))\n","    \n","    prob = result3[i][1][2].predict_proba(test)\n","    li.append((prob[:,1] >= result3[i][1][1]).astype(np.int64))\n","    \n","    prob = result4[i][1][2].predict_proba(test)\n","    li.append((prob[:,1] >= result4[i][1][1]).astype(np.int64))\n","    \n","    prob = result5[i][1][2].predict_proba(test)\n","    li.append((prob[:,1] >= result5[i][1][1]).astype(np.int64))\n","\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uL_XPrruY9vQ","executionInfo":{"status":"ok","timestamp":1645367219067,"user_tz":-540,"elapsed":353,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}},"outputId":"57ab776d-8a36-4c1c-f2fc-12aee662c1e3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["25"]},"metadata":{},"execution_count":19}],"source":["# 전체 모델의 갯수\n","len(li)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tAIGWqNtY9vQ","executionInfo":{"status":"ok","timestamp":1645367220240,"user_tz":-540,"elapsed":7,"user":{"displayName":"TITLE NO","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwJExtDaukoVv6RRiK-Ry6Anb1KigJjfXBv8cjYA=s64","userId":"00250585675516376218"}},"outputId":"9719f6bb-d78f-42e9-845a-a12683f15686"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["13"]},"metadata":{},"execution_count":20}],"source":["# 과반이상의 갯수를 출력\n","round(len(li)/2+0.51)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mc0TDW6F3YiB","outputId":"d2ac98c6-2a23-499f-89d9-25e3bd619653"},"outputs":[{"name":"stdout","output_type":"stream","text":[" 최종 결과물!!! \n"," 0.36709850346213985 \n","35816\n","13148\n"]}],"source":["result = []\n","for i in range(len(test)):\n","    score = 0\n","    for j in range(len(li)):\n","        score += li[j][i]\n","    if round(len(li)/2+0.51) <= score:\n","        result.append(1)\n","    else:\n","        result.append(0)\n","\n","print(f\" 최종 결과물!!! \")\n","print(f\" {sum(result) / len(result)} \")\n","print(len(result))\n","print(result.count(1))\n","\n","submit[\"answer\"] = result  \n","\n","# 제출 파일 저장\n","submit.to_csv('/content/drive/MyDrive/PyTorch_YearDream/2022-01-26/submission/prediction.csv', index=False)"]},{"cell_type":"markdown","source":["# 정리하자면.."],"metadata":{"id":"BVpXErrCimlL"}},{"cell_type":"markdown","source":["<ul>\n","<li>제공받은 data를 5개로 나눕니다.</li>\n","<li>이제 학습시킬 모델을 선정합니다.(여기에서는 Tree기반 모델 5개를 골랐으며, hyperparameter tuning을 진행하지 않았습니다. feature enginering이 잘 되어있어서, 괜찮은 성능을 보여준다고 생각했습니다.)</li>\n","<li>선정한 모델들을 dictionary에 담고 dictionary의 갯수는 data split한 갯수와 동일해야 합니다.</li>\n","<li>학습을 시키는데, dictionary마다 다른 데이터셋을 학습시킵니다.<br/>\n","dictionary 1번은 data1,data2,data3,data4 \t&nbsp;\t&nbsp; --> 5번 데이터 제외<br/>\n","dictionary 2번은 data1,data2,data3,data5 \t&nbsp;\t&nbsp; --> 4번 데이터 제외<br/>\n","dictionary 3번은 data1,data2,data4,data5 \t&nbsp;\t&nbsp; --> 3번 데이터 제외<br/>\n","dictionary 4번은 data1,data3,data4,data5 \t&nbsp;\t&nbsp; --> 2번 데이터 제외<br/>\n","dictionary 5번은 data2,data3,data4,data5 \t&nbsp;\t&nbsp; --> 1번 데이터 제외<br/></li>\n","<li>위와 같이 학습을 시킨다면 장점으로는 원래는 데이터가 10만개 뿐이였지만, 지금은 5등분으로 5개의 셋을 만들어내어, 40만개의 데이터셋의 학습효과를 기대해 볼 수 있습니다.</li>\n","<li>마지막으로 성능을 더 끌어올리기 위하여, hard voting을 실시합니다.<br/>\n","여기서는 그냥 과반이 넘기만 하면, 과반을 넘은 클래스로 최종결과를 제출하는 방법을 택했지만, 전체의 70% 또는 80%가 동일한 클래스로 나와야지만 제출하는 방법도 있습니다.</li>\n","<li></li>\n","<li></li>\n","</ul>"],"metadata":{"id":"Rkf0gcDPirCa"}},{"cell_type":"code","source":[""],"metadata":{"id":"9yR4eGsEkk5m"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"ML_test.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}